# -*- coding: utf-8 -*-
"""Welcome To Colab

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/notebooks/intro.ipynb
"""

import feedparser
import requests
from bs4 import BeautifulSoup
import json
import logging
import random
import time
from urllib.parse import urlparse, urljoin
from datetime import datetime
import re

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class FinancialTimesProcessor:
    def __init__(self, max_retries=3, delay_between_requests=1):
        self.ft_rss_url = "https://www.ft.com/?format=rss"
        self.max_retries = max_retries
        self.delay_between_requests = delay_between_requests

        # Enhanced transformation rules and templates
        self.emoji_map = {
            'NHS': 'ğŸ¥',
            'spending': 'ğŸ’°',
            'budget': 'ğŸ“Š',
            'review': 'ğŸ”',
            'UK': 'ğŸ‡¬ğŸ‡§',
            'election': 'ğŸ—³ï¸',
            'Labour': 'âš–ï¸',
            'Pentagon': 'ğŸ‡ºğŸ‡¸',
            'nuclear': 'â˜¢ï¸',
            'submarine': 'ğŸš¢',
            'deal': 'ğŸ¤',
            'Aukus': 'âš“ï¸',
            'Home Office': 'ğŸ›ï¸',
            'Foreign Office': 'ğŸŒ',
            'chancellor': 'ğŸ§‘âš–ï¸',
            'government': 'ğŸ›ï¸',
            'infrastructure': 'ğŸ—ï¸',
            'political': 'ğŸ—£ï¸',
            'market': 'ğŸ“ˆ',
            'stock': 'ğŸ“Š',
            'trade': 'ğŸ”„',
            'investment': 'ğŸ’¼',
            'bank': 'ğŸ¦',
            'economy': 'ğŸ“Š',
            'inflation': 'ğŸ“ˆ',
            'climate': 'ğŸŒ¡ï¸',
            'energy': 'âš¡',
            'oil': 'ğŸ›¢ï¸',
            'tech': 'ğŸ’»',
            'AI': 'ğŸ¤–',
            'crypto': 'â‚¿',
            'china': 'ğŸ‡¨ğŸ‡³',
            'europe': 'ğŸ‡ªğŸ‡º',
            'covid': 'ğŸ¦ ',
            'vaccine': 'ğŸ’‰'
        }

        self.cta_phrases = [
            "ğŸ”— Full story â†’",
            "ğŸ“– Read more â†’",
            "ğŸ’¬ Join discussion â†’",
            "ğŸ§  Analysis â†’",
            "ğŸ“Š See data â†’",
            "â±ï¸ Latest â†’",
            "ğŸ¯ Details â†’",
            "ğŸ”¥ More insights â†’"
        ]

        self.hook_phrases = [
            "ğŸš¨ Breaking:",
            "ğŸ“¢ Update:",
            "ğŸ”¥ Hot take:",
            "ğŸ’¡ Insight:",
            "ğŸ“ˆ Market move:",
            "ğŸ›ï¸ Policy shift:",
            "ğŸ” Deep dive:",
            "ğŸ¤” Question:",
            "âš¡ Just in:",
            "ğŸ¯ Key point:",
            "ğŸ“Š Data shows:"
        ]

        # Request headers for better compatibility
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }

    def fetch_ft_articles(self, max_articles=5):
        """Fetch FT articles from RSS feed with retry logic"""
        for attempt in range(self.max_retries):
            try:
                logger.info(f"Fetching RSS feed (attempt {attempt + 1}/{self.max_retries})")
                feed = feedparser.parse(self.ft_rss_url)

                if not feed.entries:
                    raise Exception("No entries found in RSS feed")

                articles = []
                for entry in feed.entries[:max_articles]:
                    # Clean and validate data
                    title = self.clean_text(entry.get('title', ''))
                    summary = self.clean_text(entry.get('summary', ''))
                    link = entry.get('link', '')

                    if title and link:  # Ensure we have minimum required data
                        articles.append({
                            'title': title,
                            'summary': summary,
                            'link': link,
                            'published': entry.get('published', ''),
                            'timestamp': datetime.now().isoformat()
                        })

                logger.info(f"Successfully fetched {len(articles)} articles from RSS")
                return articles

            except Exception as e:
                logger.warning(f"RSS fetch attempt {attempt + 1} failed: {e}")
                if attempt < self.max_retries - 1:
                    time.sleep(self.delay_between_requests * (attempt + 1))
                else:
                    logger.error("All RSS attempts failed, trying fallback scraping")
                    return self.fallback_scrape(max_articles)

    def fallback_scrape(self, max_articles):
        """Enhanced fallback HTML scraping with better selectors and error handling"""
        for attempt in range(self.max_retries):
            try:
                logger.info(f"Fallback scraping attempt {attempt + 1}/{self.max_retries}")

                response = requests.get("https://www.ft.com", headers=self.headers, timeout=10)
                response.raise_for_status()

                soup = BeautifulSoup(response.text, 'html.parser')
                articles = []

                # Try multiple selectors for better coverage
                selectors = [
                    '.o-teaser__heading a',
                    'h3 a[href*="/content/"]',
                    '.js-teaser-heading-link',
                    'a[data-trackable="heading-link"]'
                ]

                for selector in selectors:
                    items = soup.select(selector)
                    if items:
                        logger.info(f"Found articles using selector: {selector}")
                        break

                for item in items[:max_articles]:
                    try:
                        title = self.clean_text(item.get_text(strip=True))
                        if not title:
                            continue

                        # Handle relative URLs
                        href = item.get('href', '')
                        if href.startswith('/'):
                            link = urljoin("https://www.ft.com", href)
                        else:
                            link = href

                        # Try to get summary from various locations
                        summary = self.extract_summary(item)

                        articles.append({
                            'title': title,
                            'summary': summary,
                            'link': link,
                            'published': '',
                            'timestamp': datetime.now().isoformat(),
                            'source': 'scrape'
                        })

                    except Exception as e:
                        logger.warning(f"Error processing scraped item: {e}")
                        continue

                if articles:
                    logger.info(f"Successfully scraped {len(articles)} articles")
                    return articles
                else:
                    raise Exception("No articles found during scraping")

            except Exception as e:
                logger.warning(f"Scraping attempt {attempt + 1} failed: {e}")
                if attempt < self.max_retries - 1:
                    time.sleep(self.delay_between_requests * (attempt + 1))

        logger.error("All fallback attempts failed")
        return []

    def extract_summary(self, item):
        """Extract summary from various possible locations in the HTML"""
        summary = ""

        # Look for summary in parent elements
        parent = item.find_parent('article') or item.find_parent('div')
        if parent:
            # Try multiple summary selectors
            summary_selectors = [
                '.o-teaser__standfirst',
                '.teaser-standfirst',
                '.article-standfirst',
                'p',
                '.summary'
            ]

            for selector in summary_selectors:
                summary_elem = parent.select_one(selector)
                if summary_elem:
                    summary = self.clean_text(summary_elem.get_text(strip=True))
                    if summary and len(summary) > 10:  # Ensure meaningful summary
                        break

        return summary

    def clean_text(self, text):
        """Clean and normalize text content"""
        if not text:
            return ""

        # Remove extra whitespace and special characters
        text = re.sub(r'\s+', ' ', text)
        text = text.replace('\u200b', '')  # Remove zero-width space
        text = text.strip()

        # Remove common FT-specific phrases
        ft_phrases = [
            "â€” Financial Times",
            "| Financial Times",
            "- FT.com",
            "Financial Times"
        ]

        for phrase in ft_phrases:
            text = text.replace(phrase, "").strip()

        return text

    def transform_to_social_post(self, article):
        """Enhanced transformation with better content optimization"""
        title = article['title']
        summary = article['summary']

        # 1. Select relevant emojis (max 2)
        emoji = self.select_emoji(title + " " + summary)

        # 2. Add hook with controlled probability
        hook = random.choice(self.hook_phrases) if random.random() > 0.3 else ""

        # 3. Create main content
        content = self.create_optimized_content(title, summary)

        # 4. Select CTA
        cta = random.choice(self.cta_phrases)

        # 5. Assemble post with length optimization
        post = self.assemble_post(emoji, hook, content, cta)

        return post

    def create_optimized_content(self, title, summary):
        """Create optimized content considering both title and summary"""
        # Choose the best content source
        if summary and len(summary) > 20:
            # If summary is substantial, prefer it
            if len(summary) < len(title) * 1.5:  # But not if it's much longer
                base = summary
            else:
                base = title
        else:
            base = title

        # Apply content optimizations
        base = self.optimize_content(base)

        return base

    def optimize_content(self, content):
        """Apply various content optimizations"""
        # Remove redundant phrases
        redundant_phrases = [
            "it has been announced",
            "according to reports",
            "sources say",
            "it is understood"
        ]

        for phrase in redundant_phrases:
            content = re.sub(phrase, "", content, flags=re.IGNORECASE)

        # Clean up extra spaces
        content = re.sub(r'\s+', ' ', content).strip()

        return content

    def select_emoji(self, text):
        """Enhanced emoji selection with priority and context awareness"""
        text_lower = text.lower()
        matched_emojis = []

        # Priority mapping for better emoji selection
        priority_keywords = ['breaking', 'urgent', 'crisis', 'deal', 'market', 'election']

        for keyword, emoji in self.emoji_map.items():
            if keyword.lower() in text_lower:
                # Give priority to high-impact keywords
                priority = 2 if any(pk in text_lower for pk in priority_keywords) else 1
                matched_emojis.append((emoji, priority))

        if matched_emojis:
            # Sort by priority and take top 2
            matched_emojis.sort(key=lambda x: x[1], reverse=True)
            selected = [emoji for emoji, _ in matched_emojis[:2]]
            return " ".join(list(dict.fromkeys(selected)))  # Remove duplicates while preserving order

        return "ğŸ“°"  # Default news emoji

    def assemble_post(self, emoji, hook, content, cta, max_length=200):
        """Intelligently assemble post components within length constraints"""
        # Start with all components
        components = [comp for comp in [emoji, hook, content, cta] if comp]

        # Calculate initial length
        post = " ".join(components)

        if len(post) <= max_length:
            return post

        # Progressive optimization if too long
        # Step 1: Simplify CTA
        if len(cta) > 10:
            cta = "â†’"
            components[-1] = cta
            post = " ".join(components)

        if len(post) <= max_length:
            return post

        # Step 2: Remove hook if necessary
        if hook and len(post) > max_length:
            components = [comp for comp in components if comp != hook]
            post = " ".join(components)

        if len(post) <= max_length:
            return post

        # Step 3: Truncate content
        fixed_parts = [comp for comp in components if comp != content]
        fixed_length = sum(len(part) + 1 for part in fixed_parts) - 1  # -1 for last space
        available_for_content = max_length - fixed_length - 3  # 3 for "..."

        if available_for_content > 20:  # Ensure minimum content length
            content = content[:available_for_content] + "..."
            components = [emoji, content, cta] if emoji != content else [content, cta]
            post = " ".join([comp for comp in components if comp])

        return post

    def process_articles(self, max_articles=5):
        """Enhanced main processing flow with better error handling"""
        start_time = time.time()
        logger.info("Starting article processing...")

        articles = self.fetch_ft_articles(max_articles)
        if not articles:
            return {
                "error": "Could not fetch articles",
                "timestamp": datetime.now().isoformat(),
                "processing_time": time.time() - start_time
            }

        results = []
        processed_count = 0

        for i, article in enumerate(articles):
            try:
                logger.info(f"Processing article {i+1}/{len(articles)}: {article['title'][:50]}...")

                social_post = self.transform_to_social_post(article)

                result = {
                    'id': f"ft_post_{int(time.time())}_{i}",
                    'original_title': article['title'],
                    'original_summary': article['summary'],
                    'social_post': social_post,
                    'link': article['link'],
                    'length': len(social_post),
                    'published': article.get('published', ''),
                    'processed_at': datetime.now().isoformat(),
                    'source': article.get('source', 'rss')
                }

                results.append(result)
                processed_count += 1

            except Exception as e:
                logger.error(f"Error processing article {i+1}: {e}")
                # Add failed article info for debugging
                results.append({
                    'error': str(e),
                    'original_title': article.get('title', 'Unknown'),
                    'processed_at': datetime.now().isoformat()
                })
                continue

        processing_time = time.time() - start_time
        logger.info(f"Processing complete: {processed_count}/{len(articles)} articles processed in {processing_time:.2f}s")

        return {
            'results': results,
            'summary': {
                'total_articles': len(articles),
                'processed_successfully': processed_count,
                'processing_time': processing_time,
                'timestamp': datetime.now().isoformat()
            }
        }

    def save_to_json(self, data, filename=None):
        """Enhanced save function with timestamped filenames"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"ft_social_posts_{timestamp}.json"

        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
            logger.info(f"Results saved to {filename}")
            return filename
        except Exception as e:
            logger.error(f"Failed to save to {filename}: {e}")
            return None

    def validate_results(self, results):
        """Validate the processed results"""
        if isinstance(results, dict) and 'error' in results:
            return False, results['error']

        if not results.get('results'):
            return False, "No results found"

        valid_posts = [r for r in results['results'] if 'social_post' in r and len(r['social_post']) > 0]

        return len(valid_posts) > 0, f"Generated {len(valid_posts)} valid social posts"

def main():
    """Main execution function with comprehensive error handling"""
    processor = FinancialTimesProcessor()

    try:
        # Process articles
        results = processor.process_articles(max_articles=5)

        # Validate results
        is_valid, message = processor.validate_results(results)
        logger.info(f"Validation: {message}")

        if is_valid:
            # Print to console
            print(json.dumps(results, indent=2, ensure_ascii=False))

            # Save to file
            saved_file = processor.save_to_json(results)
            if saved_file:
                print(f"\nâœ… Results saved to: {saved_file}")
        else:
            print(f"âŒ Processing failed: {message}")
            print(json.dumps(results, indent=2))

    except KeyboardInterrupt:
        logger.info("Process interrupted by user")
    except Exception as e:
        logger.error(f"Unexpected error in main: {e}")
        print(f"âŒ Fatal error: {e}")

if __name__ == "__main__":
    main()